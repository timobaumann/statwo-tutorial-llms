{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9541d15b-7996-4776-8aa2-0e062aaa8f4d",
   "metadata": {},
   "source": [
    "# N-Grams: conceptually very simple language models\n",
    "N-Grams store for each CONTEXT (of size N-1), what other tokens have been observed in some training data and how often they have been observed.\n",
    "\n",
    "For example, in a given training data, \"The\" could be followed by \"y\" once, \"n\" twice, and a blank (\" \") 127 times (no other characters follow this context). From this, we can derive our probability density function: We'd say that P(y|the)=1/130, P(n|the) = 2/130, P(\" \"|the)=127/130 and all others are 0.\n",
    "\n",
    "These numbers end up being quite accurate for large amounts of training data and small N (say, up to 5 or 7).\n",
    "\n",
    "N-Grams are very compute-efficient and (if done right) reasonably memory-efficient. \n",
    "\n",
    "In the code below, we use characters as tokens; such a model can e.g. be useful for correcting spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ea2e996-06b9-4004-ae6c-112059b6bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_from_text(f, N):\n",
    "    '''\n",
    "    Given a text file (f), create all pairs of (context,next) in that file.\n",
    "    This code is ignorant of line starts and endings and considers all the data as one string\n",
    "    '''\n",
    "    context = list(f.read(N - 1))\n",
    "    last = f.read(1)\n",
    "    while last != '':\n",
    "        yield context, last\n",
    "        context = context[1:] + list(last)\n",
    "        last = f.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81551bfe-bf52-46e5-830e-b22000be6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_node(model, prefix):\n",
    "    \"\"\"from our model tree, get the node that represents the prefix (above: context) that we want to append to\"\"\"\n",
    "    node = model\n",
    "    for c in prefix:\n",
    "        if c not in node:\n",
    "            node[c] = {}\n",
    "        node = node[c]\n",
    "    return node\n",
    "\n",
    "\n",
    "def add_ngram_to_model(model, prefix, last):\n",
    "    base = get_base_node(model, prefix)\n",
    "    if last not in base:\n",
    "        base[last] = 0\n",
    "    base[last] += 1\n",
    "\n",
    "\n",
    "def estimate_model(source, N):\n",
    "    model = {}\n",
    "    with open(source, 'r', encoding='utf-8') as f:\n",
    "        ngram_source = ngrams_from_text(f, N)\n",
    "        for prefix, last in ngram_source:\n",
    "            add_ngram_to_model(model, prefix, last)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5ddf706-a0eb-4d8d-a8c5-3486698cacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 9\n",
    "source = \"data/shakespeare-en.txt\" # other files in data: lyrik-de.txt, dialoge-de.txt, merkel-de.txt\n",
    "model = estimate_model(source, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8cd28f-e8b3-4f0b-9e15-d89134246898",
   "metadata": {},
   "outputs": [],
   "source": [
    "model # run this only if you want to see a lot of output, especially for large N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0de23499-cf5a-4e35-b7fb-915d3cdcfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # we'll need this to sample from the distribution (random.choices)\n",
    "\n",
    "\n",
    "def generate(model, start):\n",
    "    start = list(start)\n",
    "    while True:\n",
    "        base = get_base_node(model, start)\n",
    "        chars, counts = zip(*base.items())\n",
    "        char_list = random.choices(population=chars, weights=counts, k=1)\n",
    "        start = start[1:] + char_list\n",
    "        yield char_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "953e93c6-f9a2-4b7d-8c6f-e04428962a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the color of the sky, the weaver. This\n",
      "    they have access to you! Hold, hold, my lord,\n",
      "    Which writ hath despis'd! traitorously as I may, indeed? Shall we burn daylight see my shame,\n",
      "    Who kept him a paper-mill. It holds his horns.\n",
      "  HOLOFERNES. I do not know me one side so, against any man in his life, and all.\n",
      "  DUCHESS OF YORK\n",
      "\n",
      "  QUEEN. What it should justice of it,  \n",
      "    For thee, and her son, as thou com'st not speak England shall ne'er pays after-love I do commit into the King\n",
      "    We shall say good night, I was no queen\n",
      "  HELENA. I do protest unto the heart out ere the devil come to her friend, and the truth,\n",
      "    That any villany!\n",
      "  Claud. I have, you\n",
      "    Immoment thrown upon his grandsire's tomb,\n",
      "    To sell again bestride him his entrails were half so kind a father.  \n",
      "  BOLINGBROKE, Duke of Norfolk, Thomas Gargrave, hast the sparrow. This\n",
      "    was again;\n",
      "    And in heaven, my blood,\n",
      "    To him that fleec'd poor part,\n",
      "    The Presented. Now 'tis here. On th' market-place. We'll give them?\n",
      "  BUC\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"What is the color of the sky\"\n",
    "print(PROMPT, end='')\n",
    "\n",
    "# limit context to the correct size\n",
    "if (len(PROMPT) > N-1):\n",
    "    CONTEXT = PROMPT[len(PROMPT)-N+1:]\n",
    "else:\n",
    "    CONTEXT = \" \" * (N-1-len(PROMPT)) + PROMPT\n",
    "i = 0\n",
    "\n",
    "# now generate 1000 characters using weighted sampling\n",
    "for c in generate(model, CONTEXT):\n",
    "    print(c, end='')\n",
    "    i += 1\n",
    "    if i > 1000:\n",
    "        break\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f944f-4044-410d-89b9-da61870eaf7d",
   "metadata": {},
   "source": [
    "Note that the N-gram fails when we query it with a context that it has never seen. (e.g., it hasn't seen \"sky?\" with question mark). \n",
    "In typical applications, we either back-off to a N-1 n-gram, or we have smoothed the model so that it is able to compute probability density functions for all possible contexts. \n",
    "\n",
    "Note also, that generation does not yet contain a \"temperature\" parameter (it always uses a temperature of 1.0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd8ae9-dd43-4e83-919e-b2f79c5031a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
